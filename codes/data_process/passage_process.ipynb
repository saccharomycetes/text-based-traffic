{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read all handbooks into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "from regex import F\n",
    "# import spacy\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "base_dir = \"../../data/hdt/handbook/raw/\"\n",
    "books = os.listdir(base_dir)\n",
    "paras = {}\n",
    "pbar = tqdm(total = 16316)\n",
    "change_cols = re.compile(r\"\\s*\\n\")\n",
    "mutil_spaces = re.compile(r\"\\s+\")\n",
    "sens_interval = re.compile(r\"\\.|\\?|\\!\")\n",
    "for book in tqdm(books):\n",
    "    state, booktype = book.split(\"-\")[0], book.split(\"-\")[1]\n",
    "    if state in paras:\n",
    "        paras[state][booktype] = []\n",
    "    else:\n",
    "        paras[state] = {}\n",
    "        paras[state][booktype] = []\n",
    "    inputpdf = PyPDF2.PdfReader(base_dir + book)\n",
    "    pages = inputpdf.pages\n",
    "    for page in pages:\n",
    "        text = page.extract_text()\n",
    "        text = re.sub(change_cols, \"\", text)\n",
    "        text = re.sub(mutil_spaces, \" \", text)\n",
    "        sens = [i + \".\" for i in re.split(sens_interval, text) if len(i) > 20]\n",
    "\n",
    "        paras[state][booktype].extend(sens)\n",
    "        pbar.update()\n",
    "\n",
    "with open(\"../../data/hdt/handbook/ori_sens.json\", \"w\")as f:\n",
    "    json.dump(paras, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=yellow>the distribution of sentence numbers (before filtering)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ak-motorcycle-940\n",
      "940\n"
     ]
    }
   ],
   "source": [
    "sens_num = 0\n",
    "for state, a in paras.items():\n",
    "    for typee, b in a.items():\n",
    "        print(state + \"-\" + typee + \"-\" + str(len(b)))\n",
    "        sens_num += len(b)\n",
    "print(sens_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the sentences by grammar checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from happytransformer import HappyTextToText, TTSettings\n",
    "\n",
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "\n",
    "args = TTSettings(num_beams=5, min_length=1)\n",
    "\n",
    "result = happy_tt.generate_text(\"grammar: ClothingCloth ing can help protect you in a cras h.\", args=args)\n",
    "\n",
    "def valid(sentence):\n",
    "    good_num = 0\n",
    "    total_num = 0\n",
    "    for i in sentence:\n",
    "        if i.isalpha() or i.isdigit() or i == \" \":\n",
    "            good_num += 1\n",
    "        total_num += 1\n",
    "    \n",
    "    return (good_num/total_num) > 0.9 and len(sentence) > 20\n",
    "pbar = tqdm(total=27483)\n",
    "\n",
    "dataset = {}\n",
    "for state, a in tqdm(paras.items()):\n",
    "    for typee, sentences in tqdm(a.items()):\n",
    "        if state in dataset:\n",
    "            dataset[state][typee] = []\n",
    "        else:\n",
    "            dataset[state] = {}\n",
    "            dataset[state][typee] = []\n",
    "        for sen in sentences:\n",
    "            sen = str(sen)\n",
    "            if valid(sen):\n",
    "                sen = happy_tt.generate_text(\"grammar: \" + sen, args=args)\n",
    "                dataset[state][typee].append(sen)\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=yellow>the distribution of sentence numbers (after filtering)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ak-motorcycle-834\n"
     ]
    }
   ],
   "source": [
    "sens_num = 0\n",
    "for state, a in dataset.items():\n",
    "    for typee, b in a.items():\n",
    "        print(state + \"-\" + typee + \"-\" + str(len(b)))\n",
    "        sens_num += len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the paragraphs (after filtering the sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "files = [str(5*i) for i in range(10)]\n",
    "all_paras = {}\n",
    "for f in files:\n",
    "    with open(\"../../data/hdt/handbook/\" + f + \".json\")as f:\n",
    "        data = json.load(f)\n",
    "    for k1, v1 in data.items():\n",
    "        all_paras[k1] = {}\n",
    "        for k, v in v1.items():\n",
    "            all_paras[k1][k] = []\n",
    "            sens_num = len(v)\n",
    "            para_num = sens_num//10 + 1\n",
    "            for i in range(para_num):\n",
    "                if i != para_num - 1:\n",
    "                    para = \" \".join(v[10*i:10*i+10])\n",
    "                else:\n",
    "                    para = \" \".join(v[10*i:-1])\n",
    "                all_paras[k1][k].append(para)\n",
    "with open(\"../../data/hdt/handbook/paras/paras.json\", \"w\")as f:\n",
    "    json.dump(all_paras, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('jrenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bac62ed733118dd8de9c1e2917899fd0f24659b3827a5fdfb2405914a98de30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
