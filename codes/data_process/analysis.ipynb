{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDD overlap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "kgs = [\"kg/\" + i for i in os.listdir(\"../../results/nli/\")]\n",
    "nlis = [\"nli/\" + i for i in os.listdir(\"../../results/nli/\")]\n",
    "dirs = [\"../../results/bdd/\" + i for i in kgs + nlis]\n",
    "all_rights = {}\n",
    "for i in [\"e\", \"c\"]:\n",
    "    all_rights[i] = {}\n",
    "    for dir in dirs:\n",
    "        with open(dir + \"/\" + i + \"/preds.json\")as f:\n",
    "            preds = json.load(f)\n",
    "            rights = set(preds[\"rights\"])\n",
    "            all_rights[i][dir.split(\"/\")[-1]] = rights\n",
    "for i in [\"e\", \"c\"]:\n",
    "    with open(\"../../results/analysis/bddoverlap\" + i + \".txt\", \"w\")as f:\n",
    "        f.write(\"\\t\")\n",
    "        for j in all_rights[i].keys():\n",
    "            f.write(j)\n",
    "            f.write(\"\\t\")\n",
    "        f.write(\"\\n\")\n",
    "        for m in all_rights[i].keys():\n",
    "            f.write(m)\n",
    "            f.write(\"\\t\")\n",
    "            for n in all_rights[i].keys():\n",
    "                overlap = str(len(all_rights[i][m].intersection(all_rights[i][n])))\n",
    "                f.write(overlap)\n",
    "                f.write(\"\\t\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDD data average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3139/3139 [02:12<00:00, 23.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40338223035867654\n",
      "0.5534353785203416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cuda\")\n",
    "pre_embeds = []\n",
    "hy_embeds = []\n",
    "with open(\"../../data/bdd/kgnli/1.jsonl\")as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        data = json.loads(line)\n",
    "        premise = data[\"premise\"]\n",
    "        hypothesis = data[\"hypothesis\"][2]\n",
    "        pembed = model.encode(premise)\n",
    "        hembed = model.encode(hypothesis)\n",
    "        pre_embeds.append(pembed)\n",
    "        hy_embeds.append(hembed)\n",
    "num_samples = len(hy_embeds)\n",
    "pre_em_sum = np.sum(pre_embeds, axis=0)\n",
    "hy_em_sum = np.sum(hy_embeds, axis=0)\n",
    "avg_simi_pre = (np.linalg.norm(pre_em_sum)**2 - num_samples)/(num_samples**2)\n",
    "avg_simi_hy = (np.linalg.norm(hy_em_sum)**2 - num_samples)/(num_samples**2)\n",
    "print(avg_simi_pre)\n",
    "print(avg_simi_hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans for clustering, save the sentences and sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "hy_embeds = []\n",
    "hypothesises = []\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cuda\")\n",
    "with open(\"../../data/bdd/kgnli/1.jsonl\")as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        data = json.loads(line)\n",
    "        hypothesis = data[\"hypothesis\"][2]\n",
    "        hembed = model.encode(hypothesis)\n",
    "        hy_embeds.append(hembed)\n",
    "        hypothesises.append(hypothesis)\n",
    "x = np.array(hy_embeds)\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(x)\n",
    "classes = [i for i in kmeans.labels_]\n",
    "sentence_clusters = {}\n",
    "for i, theclass in enumerate(classes):\n",
    "    if theclass not in sentence_clusters:\n",
    "        sentence_clusters[theclass] = [hypothesises[i]]\n",
    "    else:\n",
    "        sentence_clusters[theclass].append(hypothesises[i])\n",
    "answer_sets = {}\n",
    "for i, theclass in enumerate(classes):\n",
    "    if str(int(theclass)) not in answer_sets:\n",
    "        answer_sets[str(int(theclass))] = [i]\n",
    "    else:\n",
    "        answer_sets[str(int(theclass))].append(i)\n",
    "with open(\"../../results/bdd/analysis/ef_classes.txt\", \"w\")as f:\n",
    "    for i, sentences in sentence_clusters.items():\n",
    "        for sen in sentences[:20]:\n",
    "            f.write(sen)\n",
    "            f.write(\"\\t\")\n",
    "        f.write(\"\\n\")\n",
    "with open(\"../../results/bdd/analysis/set_classes.jsonl\", \"w\")as f:\n",
    "    f.write(json.dumps(answer_sets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the acc of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "files = [\"kg\", \"nli\"]\n",
    "ms = [os.listdir(\"../../results/bdd/\" + f) for f in files]\n",
    "models = [f + \"/\" + m + \"/\" + i for i in [\"1\", \"2\"] for f, model in zip(files, ms) for m in model ]\n",
    "base_dir = \"../../results/bdd/\"\n",
    "with open(\"../../results/bdd/analysis/set_classes.jsonl\")as f:\n",
    "    sets = json.loads(f.readlines()[0])\n",
    "all_acc = {}\n",
    "for model_dir in models:\n",
    "    all_acc[model_dir] = {}\n",
    "    pre_dir = base_dir + model_dir + \"/preds.json\"\n",
    "    with open(pre_dir)as f:\n",
    "        rights = json.load(f)[\"rights\"]\n",
    "    for cls, the_set in sets.items():\n",
    "        right_num = len(set(the_set).intersection(set(rights)))\n",
    "        all_num = len(the_set)\n",
    "        acc = right_num/all_num\n",
    "        all_acc[model_dir][cls] = acc\n",
    "\n",
    "with open(\"../../results/bdd/analysis/class_rights.txt\", \"w\")as f:\n",
    "    f.write(\"\\t\")\n",
    "    for cls in [\"accelerate\", \"Slow\", \"Stop\", \"Merge\", \"turn\"]:\n",
    "        f.write(cls)\n",
    "        f.write(\"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    for model, model_right in all_acc.items():\n",
    "        f.write(model)\n",
    "        f.write(\"\\t\")\n",
    "        for cls in [\"0\", \"1\", \"2\", \"3\", \"4\"]:\n",
    "            acc = str(model_right[cls])\n",
    "            f.write(acc)\n",
    "            f.write(\"\\t\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(\"../../data/bdd/kgnli/1.jsonl\")as f:\n",
    "    lines = f.readlines()\n",
    "    datas = [json.loads(line) for line in lines]\n",
    "for cls, members in sets.items():\n",
    "    with open(\"../../results/bdd/analysis/bdddata/\" + cls + \".jsonl\", \"w\")as f:\n",
    "        for member in members:\n",
    "            f.write(json.dumps(datas[member]))\n",
    "            f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('jrenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bac62ed733118dd8de9c1e2917899fd0f24659b3827a5fdfb2405914a98de30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
